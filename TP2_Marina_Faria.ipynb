{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de exemplo\n",
    "with open('macmorpho-dev.txt', 'r', encoding='utf-8') as arquivo:\n",
    "    text = arquivo.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentei treinar o BERT, mas sempre dava erro de memória. Pelo visto, as operações com o corpus estavam muito grandes pro meu computador aguentar (ele tem apenas 500GB de memória ), dessa forma, tive que optar por não treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_after_underscore(frase):\n",
    "    palavras = [palavra.split('_')[1] for palavra in frase.split() if '_' in palavra]\n",
    "    return palavras\n",
    "\n",
    "existing_tags = extract_tag_after_underscore(text)\n",
    "\n",
    "print(\"tags:\", existing_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_before_underscore(frase):\n",
    "    palavras = [palavra.split('_')[0] for palavra in frase.split() if '_' in palavra]\n",
    "    return palavras\n",
    "\n",
    "sentences_without_separation = extract_tag_before_underscore(text)\n",
    "sentences_temp = ['[SEP]' if (elemento == ',' or elemento == ';') else elemento for elemento in sentences_without_separation]\n",
    "sentences = ['[CLS]' if (elemento == '.') else elemento for elemento in sentences_temp]\n",
    "# Adicionando o começo da primeira sentença\n",
    "sentences.insert(0, '[CLS]')\n",
    "\n",
    "\n",
    "print(\"tags:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove duplicate tags\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(existing_tags)\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.columns = ['tag']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[df.shape[0] - 1 , 'tag'] = '[CLS]'\n",
    "df.loc[df.shape[0] - 1 , 'tag'] = '[SEP]'\n",
    "tag_values = df['tag'].values\n",
    "print(tag_values)\n",
    "len(tag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = {rotulo: indice for indice, rotulo in enumerate(tag_values)}\n",
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minha intenção era treinar o BERT, mas sempre dava erro de memória. Pelo visto, as operações com o corpus estavam muito grandes pro meu computador aguentar (ele tem apenas 500GB de memória ), dessa forma, tive que optar por utilizar um modelo pré-treinado.\n",
    "\n",
    "O primeiro modelo testado foi o 'bert-base-multilingual-cased', o qual tem o modelo pré-treinado com diversas línguas. A acurácia foi de apenas 2.93%, enquanto a precisão, 5.99%.\n",
    "\n",
    "Mudando pra as opções em português, os resultados foram melhores. Segue as opções testadas e resultados:\n",
    "- 'neuralmind/bert-base-portuguese-cased': Acurácia: 7.03%, Precisão: 20.16%\n",
    "- 'neuralmind/bert-large-portuguese-cased': Acurácia: 5.27%, Precisão: 7.29%\n",
    "- 'pierreguillou/bert-base-cased-squad-v1.1-portuguese': Acurácia: 0.59%, Precisão: 0.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Seu corpus\n",
    "corpus = text\n",
    "\n",
    "# Lista de tags\n",
    "tag2idx = tag_map\n",
    "\n",
    "# Tokenizar o corpus\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "# tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sentences, add_special_tokens=True)))\n",
    "tokens = tokenizer.encode_plus(sentences, max_length=10, padding='max_length', truncation=False, add_special_tokens=True)\n",
    "tokens = tokens[:512]\n",
    "\n",
    "# Generate attention mask\n",
    "attention_mask = tokens['attention_mask']\n",
    "\n",
    "# Converter as tags em índices\n",
    "all_tags = extract_tag_after_underscore(corpus)\n",
    "list_numbers = [tag2idx[elemento] for elemento in all_tags]\n",
    "tag_ids = list_numbers\n",
    "tag_ids = tag_ids[:512]\n",
    "\n",
    "# Converter tokens em IDs\n",
    "input_ids = tokenizer.encode(sentences, add_special_tokens=False)\n",
    "input_ids_orig = input_ids[:512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_ids)\n",
    "print(input_ids)\n",
    "print(len(attention_mask))\n",
    "print(len(tag_ids))\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com resultados baixos, decidiu utilizar métodos de otimização pra ajustar hiperparâmetros. Segue os resultados testados:\n",
    "\n",
    "- Taxa de aprendizado: o melhor resultado foi colocar lr=1e-3, no qual obteve Acurácia: 38.67%, Precisão: 31.41%. Esse resultado teve um salto em relação ao anterior, com Acurácia de 7.03% e Precisão: 20.16%.\n",
    "\n",
    "- Epochs: Essa variável foi o que mais influenciou no resultado. Anteriormente, só 3 épocas eram rodadas, mas ao setar para 50, a acurácia foi de 85.16% e a precisão: 87.09%, melhorando bastante o resultado. Entretanto, percebeu-se que o erro médio caía muito até mais ou menos a época 33, a partir dali, começou a crescer. Há grande possibilidade de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model adjustments\n",
    "from transformers import AdamW, BertConfig\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "config.hidden_dropout_prob = 0.4  # Exemplo de dropout na camada de embedding\n",
    "config.attention_probs_dropout_prob = 0.4  # Exemplo de dropout nas camadas de atenção\n",
    "\n",
    "# Carregar o modelo BERT para token classification\n",
    "model = BertForTokenClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=len(tag2idx))\n",
    "model.classifier.dropout = nn.Dropout(0.4)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "\n",
    "# Dados para DataLoader\n",
    "labels = all_tags[:512]\n",
    "labels = [tag2idx[label] for label in labels]\n",
    "\n",
    "input_ids = torch.tensor(input_ids_orig)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "attention_mask = attention_mask.unsqueeze(0)\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Treinamento\n",
    "epochs = 50  # Ajuste conforme necessário\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids_orig)\n",
    "len(tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a inferência\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor([input_ids_orig]), labels=torch.tensor([tag_ids]))\n",
    "\n",
    "# Obter as previsões\n",
    "predictions = torch.argmax(outputs.logits, dim=2).tolist()[0]\n",
    "\n",
    "# Mapear os índices de volta para tags\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "predicted_tags = [idx2tag[idx] for idx in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o resultado\n",
    "tokens_from_ids = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "for token, tag in zip(tokens_from_ids, predicted_tags):\n",
    "    print(f\"Token: {token}\\t\\tPOS: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calcular acurácia\n",
    "accuracy = accuracy_score(all_tags[:512], predicted_tags)\n",
    "precision = precision_score(all_tags[:512], predicted_tags, average='weighted')\n",
    "recall = recall_score(all_tags[:512], predicted_tags, average='weighted')\n",
    "f1 = f1_score(all_tags[:512], predicted_tags, average='weighted')\n",
    "\n",
    "print(f'Acurácia: {accuracy * 100:.2f}%')\n",
    "print(f'Precisão: {precision * 100:.2f}%')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "print(f'F1-score: {f1 * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
